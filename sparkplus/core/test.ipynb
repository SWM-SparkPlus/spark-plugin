{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/taypark/DEV/apache-spark/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/10/12 05:16:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[('spark.app.name', 't'), ('spark.app.startTime', '1633983376848'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.sql.warehouse.dir', 'file:/Users/taypark/Repositories/spark-plugin/sparkplus/core/spark-warehouse'), ('spark.driver.extraClassPath', '/Users/taypark/Repositories/spark-plugin/resource/mysql-connector-java-8.0.26/mysql-connector-java-8.0.26.jar'), ('spark.driver.port', '54511'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.maxResultSize', '0'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.memory', '14g'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.execution.arrow.pyspark.enabled', 'true'), ('spark.app.id', 'local-1633983377979')]\n"
     ]
    }
   ],
   "source": [
    "from base import SPDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "ss_builder = SparkSession.builder.appName('t')\n",
    "\n",
    "ss_builder.config('spark.driver.extraClassPath',\n",
    "                  '/Users/taypark/Repositories/spark-plugin/resource'\n",
    "                  '/mysql-connector-java-8.0.26/mysql-connector-java-8.0.26.jar')\\\n",
    "    .config('spark.driver.memory', '14g')\\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "    .config('spark.driver.maxResultSize', 0)\n",
    "\n",
    "ss = ss_builder.getOrCreate()\n",
    "\n",
    "# print(ss.sparkContext.getConf().getAll())\n",
    "\n",
    "gyeonggi_table_df = SPDataFrame.get_db_df_by_tablenames(ss, ['integrated_address_gyeonggi'],\n",
    "                                          driver='com.mysql.cj.jdbc.Driver',\n",
    "                                          url='jdbc:mysql://localhost:3306/sparkplus',\n",
    "                                          user='root',\n",
    "                                          password='sparkplus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manage_number: string (nullable = true)\n",
      " |-- roadname_code: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- sido: string (nullable = true)\n",
      " |-- sigungu: string (nullable = true)\n",
      " |-- eupmyeondong: string (nullable = true)\n",
      " |-- bupjungli: string (nullable = true)\n",
      " |-- roadname: string (nullable = true)\n",
      " |-- is_basement: string (nullable = true)\n",
      " |-- building_primary_number: integer (nullable = true)\n",
      " |-- building_secondary_number: integer (nullable = true)\n",
      " |-- bupjungdong_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gyeonggi_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDataFrame(gyeonggi_table_df).address_to_h3(addr_col_name='is_basement').show()\n",
    "import geopandas as gpd\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# gyeonggi_table_df.limit(1)\\\n",
    "#     .withColumn('point', lit(gpd.points_from_xy(37.3211047, 126.9889655))).show()\n",
    "\n",
    "PREFIX = (37.32, 126.98)\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "xy_box = []\n",
    "for i in range(50):\n",
    "    x, y = PREFIX\n",
    "    px, py = str(x), str(y)\n",
    "    px, py = px + str(fake.random_int(0, 99999)), py + str(fake.random_int(0, 99999))\n",
    "    xy_box.append((px, py))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         PNU     JIBUN BCHK  SGG_OID COL_ADM_SE  \\\n",
      "0        4111710300101670003    167-3전    1   270697      41110   \n",
      "1        4111113800101980001    198-1전    1   270698      41110   \n",
      "2        4111710300201190028  산119-28임    1   270699      41110   \n",
      "3        4111710300109070001    907-1잡    1   698166      41110   \n",
      "4        4111710300101770017   177-17임    1   270701      41110   \n",
      "...                      ...       ...  ...      ...        ...   \n",
      "5078153  4183040033101950006   195-6 전    1  1892811      41830   \n",
      "5078154  4183039521200440024  산44-24 임    1  1893383      41830   \n",
      "5078155  4183036023102440000      244전    1  1885066      41830   \n",
      "5078156  4183036023102440004   244-4 전    1  1885067      41830   \n",
      "5078157  4183036023102440003   244-3 전    1  1885068      41830   \n",
      "\n",
      "                                                  geometry  \n",
      "0        POLYGON ((127.05529 37.28866, 127.05533 37.288...  \n",
      "1        POLYGON ((127.01543 37.32614, 127.01547 37.326...  \n",
      "2        POLYGON ((127.05120 37.28951, 127.05120 37.289...  \n",
      "3        POLYGON ((127.03676 37.29320, 127.03723 37.294...  \n",
      "4        POLYGON ((127.05132 37.28945, 127.05170 37.289...  \n",
      "...                                                    ...  \n",
      "5078153  POLYGON ((127.59218 37.54486, 127.59216 37.544...  \n",
      "5078154  POLYGON ((127.64311 37.46407, 127.64309 37.464...  \n",
      "5078155  POLYGON ((127.65476 37.52167, 127.65476 37.521...  \n",
      "5078156  POLYGON ((127.65500 37.52165, 127.65498 37.521...  \n",
      "5078157  POLYGON ((127.65516 37.52165, 127.65516 37.521...  \n",
      "\n",
      "[5078158 rows x 6 columns]\n",
      "21/10/12 08:15:01 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 928702 ms exceeds timeout 120000 ms\n",
      "21/10/12 08:15:01 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "coord_df = ss.createDataFrame(xy_box)\n",
    "coord_df = coord_df.withColumnRenamed('_1', 'lat').withColumnRenamed('_2', 'lng')\n",
    "\n",
    "import os \n",
    "PARQUET_PATH = os.getcwd() + '/../../resource/Gyeonggi.parquet'\n",
    "\n",
    "gyeonggi_gdf = gpd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "print(gyeonggi_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coord_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z5/59xjw6ps4m95mnplymd3q3hc0000gn/T/ipykernel_49531/2756843327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 개발용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpandas_coord_to_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(pandas_coord_to_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coord_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 개발용\n",
    "\n",
    "pandas_coord_to_df = coord_df.toPandas()\n",
    "\n",
    "# print(pandas_coord_to_df)\n",
    "print(pandas_coord_to_df)\n",
    "\n",
    "# point_sdf_to_geodataframe = gpd.GeoDataFrame(coord_df, geometry=gpd.points_from_xy(pandas_coord_to_df.lat, pandas_coord_to_df.lng))\n",
    "\n",
    "# print(point_sdf_to_geodataframe)\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "# for i in point_sdf_to_geodataframe.index:\n",
    "#     for j in gyeonggi_gdf.index:\n",
    "#         if gyeonggi_gdf.geometry[j].contains(point_sdf_to_geodataframe[i]):\n",
    "#             temp_list.append(gyeonggi_gdf.EMD_CD)\n",
    "\n",
    "print(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
